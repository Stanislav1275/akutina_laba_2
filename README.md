# 1. Введение

Задача themporal human detecter — определение фотографий, сделанных с тепловизорных камер, в которых обнаружены люди и их пространственных меток. Задача актуальна в сфере безопасности, охраны, военной промышленности, научных исследованиях. Классические подходы к подсчёту делятся на детекционные, регрессионные и оценку плотности (density maps). За последние годы точность и устойчивость существенно выросли благодаря глубоким CNN, новым функциям потерь и появлению крупных датасетов (ShanghaiTech, UCF-QNRF, UCSD, др.). Одновременно сохраняются инженерные вызовы: окклюзии, перспектива/масштаб, неравномерная плотность, шумы сцены.

Цель проекта — спроектировать реалистичную для учебной команды систему, которая:

- считает IN/OUT через виртуальные линии;

- принимает массив фотографий и отдаёт те, где обнаружены люди, вместе с выделение человека.

- использует готовые предобученные модели, минимизируя порог входа.

# 2. Функциональные требования

Источники данных: видеофайлы.

**Обработка кадров**: дискретизация 6–10 fps; базовый препроцессинг (масштабирование, нормализация).

**Детекция людей**: выделение bbox класса person с оценкой уверенности.

**Веб-панель**: приложение для взаимодействия с системой, просмотром фотографий.

# 3. Нефункциональные требования
### 1. Производительность

Система должна эффективно работать при обработке видеозаписей различной длины и разрешения.
Основные показатели:

- скорость обработки — не менее 5 кадров в секунду при разрешении 1280×720;

- время предварительной загрузки видео длительностью до 30 минут — не более 10 секунд;

- время генерации отчёта (тепловая карта, статистика) — не более 15 секунд после завершения анализа.
  
- Программа должна стабильно работать при последовательной обработке нескольких видеозаписей без ручного вмешательства.

---
### 2. Масштабируемость

Архитектура системы должна позволять:

- добавление новых видеофайлов в очередь без остановки работы программы;

- обработку видео разной длительности и разрешения без изменения кода;

- перенос системы на более мощное оборудование или в облачную среду без модификации логики.
  
-  Производительность при увеличении объёма данных должна снижаться линейно, без критических задержек.

---

### 3. Эргономичность

Интерфейс приложения должен быть простым и понятным для пользователя.
Требуется:

- удобная загрузка файлов (drag-and-drop или выбор из проводника);

- визуальное отображение прогресса обработки;

- логичная структура навигации и элементов интерфейса;

- наглядная визуализация результатов — графики, тепловые карты, статистика. Пользователь должен уметь запустить полный цикл анализа без обращения к документации.

---

### 4. Надёжность

Система должна обеспечивать устойчивую работу при типичных ошибках и сбоях.
Требуется:

- сохранение промежуточных результатов при прерывании обработки;

- возможность повторного запуска с места остановки;

- корректное завершение при повреждённом видеофайле;

- отсутствие утечек памяти и зависаний при длительном анализе.
  
Среднее время безотказной работы должно составлять не менее 24 часов непрерывной эксплуатации.

---
### 5. Безопасность

Система должна обеспечивать защиту данных пользователей и результатов анализа.
Основные меры:

- обработка видео локально, без передачи в сеть;

- хранение только числовых и графических результатов (без исходного видео при необходимости приватности);

- контроль доступа к сохранённым отчётам;

- защита файлов отчётов и базы данных от несанкционированного изменения.
  
В случае расширения до сетевой версии необходимо предусмотреть шифрование соединений (TLS) и авторизацию пользователей.

# 4. Архитектура приложения

<img width="1534" height="500" alt="image" src="https://github.com/user-attachments/assets/5078c931-27d9-4c0c-8f63-2a75a4f1e648" />

### 1. Идея

Пользователь в веб-интерфейсе загружает видео. Бэкенд принимает файл, запускает офлайн-обработку (разбиение на кадры → детекция/трекинг → подсчёты/теплокарта), складывает артефакты в папку задачи и отдаёт фронту итоговый JSON/CSV/PNG. Постоянная БД не требуется: метаданные задачи — в manifest.json внутри папки задачи.

---

### 2. Компоненты

**Frontend (отдельно):**

- Загрузка файла + прогресс-бар.

- Экран статуса задачи.

- Экран результатов: карточки метрик (IN/OUT, пиковые интервалы), графики по времени, отображение heatmap.png, кнопки «скачать CSV/PNG».

- Технологии: React/Next.js (или чистый React/Vite). Запросы — fetch к /api/....

**Backend (отдельно):**

- HTTP API: приём файла, статус, выдача результатов и артефактов.

- Обработчик видео (воркер) в отдельном процессе/потоке (чтобы не блокировать API).

- Хранилище задач — файловая система:

- /jobs/<job_id>/input.mp4

- /jobs/<job_id>/manifest.json (status: queued|running|done|failed, тайминги, параметры)

- итоговые файлы (JSON/CSV/PNG/опц. видео превью).

Технологии: Python + FastAPI; очередь — простая in-memory/multiprocessing (для 1–2 одновременных задач достаточно). Контейнеризация — Docker (фронт и бэк отдельными контейнерами).

---



### 3. Потоки данных и форматы

- summary.json: сводные метрики (IN/OUT по окнам, длительность, частота кадров анализа, параметры линии/ROI).

- events.csv: ts, direction(in|out), track_id, confidence.

- tracks.csv: ts, track_id, cx, cy.

- heatmap.png: итоговая тепловая карта (или heatmap.npy для фронтового рендера — опционально).

---

### 4. Нефункциональные акценты (под офлайн)

- **Размеры и таймауты**: лимит на файл (например, 1–2 ГБ), chunked upload.

- **Прогресс**: фронт опрашивает /status (например, каждые 2–3 с).

- **Ретенция**: авточистка папок /jobs/* старше N дней/ГБ.

- **Безопасность**: в учебной версии — CORS + размер файла + MIME-валидация; при публикации — auth и ограничение частоты.

- **Масштабирование**: если задач много — вынос артефактов в объектное хранилище (S3/MinIO) и запуск нескольких воркеров/реплик API (job-папки с общим томом). БД по-прежнему не обязательна.

---

### 5. Почему без БД — нормально

- Весь результат детерминирован и самодостаточен в папке задачи;

- Метаданные — в manifest.json, сводка — в summary.json;

- Для лабораторной это сокращает сложность и ускоряет разработку;

- При росте требований можно «просто» добавить БД (PostgreSQL) для истории/поиска по задачам — API останется тем же.

<img width="1280" height="1000" alt="image" src="https://github.com/user-attachments/assets/6c833476-9f69-4292-a0b9-0f9b0bad1791" />

# 5. Архитектура интеллектуальной системы

### Цель и контекст

Приложение выполняет пакетную постобработку видеозаписей: для каждого файла извлекаются кадры с заданным шагом, на кадрах выполняется детекция людей и вычисляются события пересечения контрольной линии (IN/OUT), траектории и тепловые карты. Результаты сохраняются в БД и/или файлах отчётов.

### 1. Общее сравнение подходов

| **Подход** | **Механизм** | **Преимущества** | **Недостатки** |
| ---        | ---          | ---              | ---            |
| **Detection-based (на основе CNN)** |     Использует свёрточную нейросеть для детекции отдельных людей (например, YOLO, SSD, Faster R-CNN).      |          Высокая точность при умеренной плотности, визуально понятные результаты. |          Снижение точности при сильных перекрытиях или высокой плотности толпы. |
| **Regression-based**   | Извлекает признаки (текстура, площадь, яркость) и оценивает общее число людей с помощью регрессионной модели.     | Простая реализация, быстрое обучение.    |   Теряет пространственную информацию, плохо переносится между сценами. |
| **Density-map-based**    | Генерирует карту плотности (heatmap), где интенсивность соответствует числу людей в области кадра.       | Учитывает распределение и плотность, устойчив к окклюзиям.      | Требует сложной разметки (точки голов), обучение занимает много ресурсов. |

Из этих трёх подходов именно **детекционные методы на основе свёрточных нейронных сетей (CNN)** сегодня считаются **наиболее универсальными и интерпретируемыми**.
Это подтверждается в ряде публикаций (например, Akshita Patwal et al., 2023), где указано, что детекционные CNN-модели (YOLO, Faster R-CNN, SSD) обеспечивают **высокую точность на сценах с умеренной плотностью**, что делает их оптимальными для инженерных прототипов и прикладных решений.

### 2. Объективные преимущества детекционного CNN-подхода

| Критерий | Detection-based (YOLO, SSD) | Density-map (SRNet, CRNet и др.) | Regression-based |
|:---|:---|:---|:---|
| **Интерпретируемость** | Каждый человек визуализируется рамкой, легко проверить результат | Результат — плотность, без точных позиций | Даёт только число, без визуальной валидации |
| **Необходимость обучения** | Можно использовать предобученные модели (COCO, CrowdHuman) | Требуется обучение на размеченных картах плотности | Требует обучающего набора с конкретной сценой |
| **Требования к вычислительным ресурсам** | Умеренные — работает на CPU | Очень высокие (GPU + большие батчи) | Низкие, но низкая точность |
| **Точность на разреженных сценах** | Высокая (Precision > 0.9) | Снижается из-за усреднения плотности | Средняя |
| **Применимость в инженерных задачах** | Подходит для прототипов и систем видеонаблюдения | Чаще используется в научных экспериментах | Ограничена |
| **Устойчивость к изменению сцены** | Хорошая — CNN видит контекст | Низкая, требует адаптации | Плохо переносится между сценами |

### 3. Практическая применимость YOLO

Модель **YOLO (You Only Look Once)** представляет собой **универсальный CNN-детектор**, который объединяет высокую скорость и точность.  
Согласно открытым бенчмаркам:
- **YOLOv5 / YOLOv8** достигает *mAP (mean average precision)* 0.5–0.7 на наборе COCO при скорости более **100 FPS на GPU** и **8–12 FPS на CPU**.
- При применении к задаче подсчёта людей YOLO стабильно выделяет объекты даже в кадрах средней плотности (до 30 человек) без дообучения.

| Модель        | mAP@50-95 | Параметры ~ | Примерная скорость  |
| ------------- | --------- | ----------- | ------------------- |
| YOLOv5n (640) | ~ 28.0    | ~2.6 M      | ~73.6 ms (CPU ONNX) |
| YOLOv5s (640) | ~ 37.4    | ~9.1 M      | ~120.7 ms           |
| YOLOv8n (640) | ~ 37.3    | ~3.2 M      | ~80.4 ms            |
| YOLOv8s (640) | ~ 44.9    | ~11.2 M     | ~128.4 ms           |
| YOLOv8m (640) | ~ 50.2    | ~25.9 M     | ~234.7 ms           |

YOLO используется в реальных системах:
- **Crowd Counting Toolkit (OpenCV AI)** — детекция людей с помощью YOLO;
- **Smart Retail Analytics (Intel)** — анализ посетителей магазинов;
- **Transportation Flow Analysis (NVIDIA DeepStream)** — анализ трафика.

Таким образом, YOLO доказала эффективность **в практических инженерных приложениях**, а не только в академических тестах.

---

### 4. Соответствие возможностям проекта

| Ограничение учебного проекта | Реакция детекционного подхода |
|:---|:---|
| Нет GPU / ограниченные ресурсы | Лёгкие варианты YOLO (v5n, v8n) работают на CPU с приемлемой скоростью (5–10 кадров/с). |
| Нет размеченного датасета | Можно использовать предобученные веса на COCO без дополнительного обучения. |
| Требуется визуальная проверка результатов | YOLO визуализирует каждого человека, что упрощает отладку. |
| Нужно получить тепловые карты и статистику | Детектор даёт координаты, которые легко преобразовать в плотностную карту. |
| Ограниченное время на реализацию | Интеграция YOLO через библиотеку `ultralytics` или `torchvision` занимает < 100 строк кода. |

---

### 5. Научное обоснование выбора

В обзоре (*Patwal et al., 2023*) показано, что **детекционные CNN-подходы** сохраняют лидирующие позиции по точности и устойчивости для низко- и среднеплотных сцен.

- **Detection-based methods** имеют высокую точность для разреженных толп и минимальные вычислительные требования.  
- **Density estimation**-методы лучше работают в сверхплотных сценах, но требуют значительных ресурсов.  
- Обучение таких моделей требует тысяч размеченных изображений и GPU, что нецелесообразно для лабораторного проекта.

Таким образом, выбор **Detection-based CNN (YOLO)** объективно обоснован:  
он сочетает **современные технологии глубокого обучения**, **высокую точность**, **простоту внедрения** и **доказанную практическую эффективность**.

---

**Вывод:**  
Детекционный подход на основе свёрточных нейронных сетей — оптимальное решение для задачи подсчёта посетителей и анализа трафика.  
Он сочетает:
- научную обоснованность (подтверждено в обзорах и бенчмарках);
- инженерную реализуемость в учебных условиях;
- достаточную точность и интерпретируемость результатов.

Поэтому ядром интеллектуальной системы выбрана модель **YOLO**, выполняющая обнаружение и отслеживание людей на кадрах видеозаписи.

## 6. Выбор технологий разработки

### 1. Принципы выбора
При выборе технологий проект опирался не на личные предпочтения, а на **объективные критерии**:

1. **Соответствие функционалу** — возможность обрабатывать видеофайлы офлайн, запускать нейронную сеть и визуализировать результаты.
2. **Доступность и воспроизводимость** — технологии должны работать на обычном ноутбуке без GPU и без сложного развёртывания.
3. **Поддержка экосистемы ИИ** — наличие готовых библиотек для компьютерного зрения и глубокого обучения.
4. **Простота интеграции фронтенда и бэкенда** — стандартные API-механизмы, кросс-браузерная совместимость.
5. **Открытая лицензия и широкое сообщество** — чтобы проект можно было дорабатывать и использовать в дальнейшем.

Исходя из этих факторов, был выбран стек **Python + FastAPI (backend)** и **React + Tailwind CSS (frontend)**.

---

### 2. Обоснование выбора Backend-технологий

| Технология | Альтернатива | Обоснование |
|:---|:---|:---|
| **Python 3.11 + FastAPI** | Node.js (Express), Go (Fiber) | Python — де-факто стандарт для задач компьютерного зрения и ИИ. Имеет прямую интеграцию с PyTorch и OpenCV. FastAPI показывает производительность, сравнимую с Node.js, но при этом поддерживает асинхронность и простую типизацию. В отличие от Flask, FastAPI обеспечивает автоматическую генерацию документации API и высокую скорость отклика (≈ 35–40 тыс req/s). |
| **PyTorch + Ultralytics YOLOv8** | TensorFlow (Object Detection API), Detectron2 | PyTorch — основа большинства современных CNN-архитектур. Модель YOLO v8 обеспечивает высокую точность (mAP ≈ 0.7 на COCO) при низких требованиях к вычислениям и доступна в виде готового пакета `ultralytics`. Для TensorFlow потребовалась бы ручная настройка модели и больше ресурсов; Detectron2 сложен для установки и не оптимизирован под CPU-инференс. |
| **OpenCV + NumPy** | FFmpeg CLI, Scikit-image | OpenCV позволяет декодировать видео, извлекать кадры и визуализировать результаты в одном API. NumPy обеспечивает матричные операции и удобное хранение координат объектов. Эти библиотеки полностью совместимы с PyTorch и не требуют GPU. |
| **Matplotlib / Seaborn** | Plotly, Bokeh | Для офлайн-отчётов достаточно статичных изображений (PNG). Matplotlib лёгкий, не требует сервера и интегрируется с OpenCV при построении тепловых карт. |
| **JSON / CSV / PNG** | PostgreSQL, MongoDB | Для офлайн-анализа нет необходимости в постоянной БД. Форматы JSON и CSV обеспечивают читаемость, кроссплатформенность и лёгкий импорт в аналитику (Excel, Pandas). |

> **Вывод:** Python + FastAPI + YOLO — это минимально достаточный и в то же время профессиональный стек для систем офлайн-видеоаналитики. Он доказал эффективность в индустриальных продуктах (OpenCV AI Toolkit, Intel Edge Analytics) и обеспечивает баланс между точностью, скоростью и простотой.

---

### 3. Обоснование выбора Frontend-технологий

| Технология | Альтернатива | Обоснование |
|:---|:---|:---|
| **React + Vite** | Angular, Vue, Django Templates | React имеет наибольшее сообщество и поддержку экосистемы визуальных библиотек (Chart.js, D3.js). Vite обеспечивает быструю сборку и обновление интерфейса без перезапуска. Angular избыточен для простого дашборда, Vue — меньше инструментов для визуализации. |
| **Tailwind CSS** | Bootstrap, Material UI | Tailwind позволяет быстро собрать адаптивный интерфейс без тяжёлых компонентов. Для лабораторного проекта это минимизирует зависимость и ускоряет разработку. |
| **Chart.js / D3.js** | ECharts, Plotly.js | Chart.js прост в интеграции с React, не требует серверного рендеринга. Поддерживает линейные графики, гистограммы и тепловые карты — всё, что нужно для статистики и анализа трафика. |
| **Axios / Fetch API** | GraphQL, WebSocket | REST-взаимодействие проще и надёжнее для асинхронных запросов статуса обработки. WebSocket избыточен, т.к. видео анализируется офлайн, а не в реальном времени. |

> **Вывод:** React + Tailwind CSS обеспечивает современный, отзывчивый интерфейс, совместимый со всеми браузерами, и требует минимум настройки. Такой стек используется в реальных аналитических дашбордах (например, Metabase UI, Streamlit React-front).

---

### 4. Сводное сравнение стеков

Для выбора технологий были рассмотрены три типовых стека:
1. **Python + FastAPI / React + Tailwind** — ориентирован на анализ данных и офлайн-обработку;
2. **Node.js / Express / Vue.js** — типичный веб-стек для интерактивных приложений;
3. **Java Spring Boot / Angular** — промышленный стек для корпоративных систем.

| Критерий | Python + FastAPI / React + Tailwind | Node.js / Express / Vue.js | Java Spring / Angular | Обоснование |
|:---|:---|:---|:---|:---|
| **Средняя скорость отклика API (GET/POST)** | ~3–5 мс на запрос | ~4–6 мс | ~10–15 мс | Тесты [TechEmpower 2024]: FastAPI близок к Express по латентности, Java требует JVM-инициализации. |
| **Порог входа в разработку** | Низкий | Средний | Высокий | Python и JS проще, чем Java; Spring требует конфигурации и аннотаций. |
| **Интеграция с нейронными сетями** | Прямая (PyTorch, OpenCV, YOLO) | Через внешние REST-сервисы | Ограниченная (Java ML-библиотеки устаревшие) | В экосистеме Python доступны готовые модели и веса. |
| **Потребление памяти при API-нагрузке** | 60–100 МБ | 100–130 МБ | 200+ МБ | Python и Node легче JVM-стека. |
| **Поддержка офлайн-видеоаналитики** | Да (cv2, numpy, matplotlib) | Нет (нужно C++-обёртки или Python-скрипты) | Нет (только внешние сервисы) | OpenCV и NumPy доступны только в Python. |
| **Наличие готовых UI-библиотек** | Tailwind, Chart.js | Vuetify, Chart.js | Angular Material | Все поддерживают графики, но React/Tailwind проще подключить. |
| **Документация и комьюнити** | Очень развитое | Развитое | Среднее | PyTorch, FastAPI и React имеют активное обновление и большие сообщества. |
| **Совместимость с CPU-средой (без GPU)** | Полная | Частичная | Частичная | YOLOv8 в Python поддерживает CPU-инференс; в других стекax — только через API. |
| **Среднее время запуска проекта (hello world + REST)** | 2–3 минуты | 3–4 минуты | 10–15 минут | FastAPI и Express требуют минимальной конфигурации, Spring — полноценного проекта. |
| **Лицензия и открытость** | MIT / Apache-2 | MIT | Apache-2 | Все свободные, но Python-пакеты проще распространять. |

---

#### Анализ результатов

- **Python + FastAPI / React + Tailwind** обеспечивает лучшее сочетание производительности, простоты и совместимости с инструментами машинного обучения.  
  Это единственный стек, где:
  - нейросетевой анализ (YOLO, PyTorch) интегрируется *внутри приложения*, без внешнего API;
  - поддерживается офлайн-обработка видео прямо в коде;
  - запуск возможен без установки СУБД или GPU.
- **Node.js / Express** подходит для веб-части, но не для анализа видео: придётся писать Python-мост или вызывать внешние сервисы.  
- **Java / Angular** избыточен по сложности и ресурсам для учебного проекта, где требуется лёгкая сборка и частые изменения кода.

---

#### Итог

| Критерий выбора | Лучшая технология | Обоснование |
|:---|:---|:---|
| Анализ изображений и видео | **Python (OpenCV, PyTorch)** | Единственная среда с полным набором CV-библиотек. |
| Разработка REST API | **FastAPI** | Современный, быстрый и типизированный фреймворк. |
| Простая визуализация результатов | **React + Chart.js** | Поддерживает интерактивные графики и обновление без перезагрузки. |
| Адаптивный дизайн интерфейса | **Tailwind CSS** | Ускоряет вёрстку и исключает лишние зависимости. |

Таким образом, стек **Python + FastAPI / React + Tailwind CSS** выбран на основании объективных технических показателей:
- минимальная зависимость от внешних систем,
- высокая производительность при низких ресурсах,
- прямая интеграция с библиотеками глубокого обучения,
- оптимальная сложность для учебного проекта и воспроизводимость на любом ПК.

---

### 5. Заключение

Выбранный стек **Python (FastAPI, PyTorch, OpenCV)** + **React (Tailwind CSS, Chart.js)**  
объективно удовлетворяет всем критериям для разработки системы подсчёта посетителей:

- **Доказанная эффективность** в инженерных и исследовательских проектах.  
- **Доступность и простота** — установка за несколько минут, запуск на CPU.  
- **Баланс точности и производительности** для офлайн-анализа видео.  
- **Модульность** — независимый фронт и бэк с REST-взаимодействием.  
- **Масштабируемость** — возможность позже добавить БД, облачное хранилище или обработку в потоковом режиме без смены архитектуры.

Таким образом, выбранные технологии обеспечивают **объективно оптимальное сочетание функциональности, устойчивости и трудозатрат** для учебного и прикладного уровня разработки.

### 6. Источники

1. Crowd counting analysis using deep learning: a critical review [Электронный ресурс] // Procedia Computer Science. – 2023. – Т. 218. – С. 2448–2458. – Режим доступа: https://www.sciencedirect.com/science/article/pii/S187705092300220X
 (дата обращения: 14.10.2025).

2. Deep Learning Based Efficient Crowd Counting System [Электронный ресурс] / S. R. Bhamare, A. Patil, P. Jagtap и др. – Режим доступа: https://www.researchgate.net/publication/380738258_Deep_Learning_Based_Efficient_Crowd_Counting_System
 (дата обращения: 14.10.2025).

3. Crowd Counting Method Based on Convolutional Neural Network with Global Density Feature [Электронный ресурс] / J. Cao, Y. Wang, Z. Zhao, F. Su. – IEEE Access. – 2019. – Режим доступа: https://www.researchgate.net/publication/334287312_Crowd_Counting_Method_Based_on_Convolutional_Neural_Network_With_Global_Density_Feature
 (дата обращения: 14.10.2025).

4. Redmon J., Divvala S., Girshick R., Farhadi A. You Only Look Once: Unified, Real-Time Object Detection [Электронный ресурс]. – arXiv preprint arXiv:1506.02640, 2016. – Режим доступа: https://arxiv.org/abs/1506.02640
 (дата обращения: 14.10.2025).

5. CASA-Crowd: A Context-Aware Scale Aggregation CNN-Based Crowd Counting Technique [Электронный ресурс] / S. Arif, A. Zafar, M. Alam и др. – IEEE Access. – 2020. – Режим доступа: https://ieeexplore.ieee.org/document/8935161
 (дата обращения: 14.10.2025).
  
